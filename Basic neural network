{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPWlisYnklpDzLphyNa3IRe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshemani/python-files/blob/master/Basic%20neural%20network\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbfhHuWCDeeZ"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Oct  2 20:24:49 2020\n",
        "\n",
        "@author: mahesh.emani\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self,num_inputs=5,num_hidden=[5,5],num_outputs=5):\n",
        "        self.num_inputs=num_inputs\n",
        "        self.num_hidden=num_hidden\n",
        "        self.num_outputs=num_outputs\n",
        "        \"\"\"\n",
        "        all the parameters indicate the no.of neurons in each layer\n",
        "        num_hidden is a list. Length of list is the num of hidden layers\n",
        "        and the elements indicate the no.of neurons in each layer\n",
        "        \"\"\"\n",
        "        neu_per_layer= [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
        "        \n",
        "        #calculate the weights matrices for all the stages\n",
        "        weights = []\n",
        "        for i in range(len(neu_per_layer)-1):\n",
        "            w=np.random.rand(neu_per_layer[i],neu_per_layer[i+1])\n",
        "            weights.append(w)\n",
        "        self.weights = weights\n",
        "        \n",
        "        #create activation matrix\n",
        "        a_matrix=[]\n",
        "        for i in neu_per_layer:\n",
        "            a=np.zeros(i)\n",
        "            a_matrix.append(a)\n",
        "        self.a_matrix=a_matrix\n",
        "        \n",
        "        #create derivates matrix\n",
        "        derivatives = []\n",
        "        for i in range(len(neu_per_layer)-1):\n",
        "            d=np.random.rand(neu_per_layer[i],neu_per_layer[i+1])\n",
        "            derivatives.append(d)\n",
        "        self.derivatives = derivatives\n",
        "    \n",
        "    \n",
        "        \n",
        "    def forward_propagation(self,inputs):\n",
        "        \"\"\"\n",
        "        this function uses the input matrix from the user \n",
        "        and applies the weights matrix itertively until output layer is obtained\n",
        "        we do not store the intermediate neuron values\n",
        "        \"\"\"\n",
        "        a_matrix=inputs\n",
        "        self.a_matrix[0]=inputs\n",
        "        for i,w in enumerate(self.weights):\n",
        "            h_matrix=np.dot(a_matrix,w)\n",
        "            #update the a_matrix with new layer inputs\n",
        "            a_matrix=self._sigmoid(h_matrix)\n",
        "            self.a_matrix[i+1]=a_matrix\n",
        "        return a_matrix\n",
        "    \n",
        "    def back_propagation(self,error):\n",
        "        \n",
        "    \n",
        "    def _sigmoid(self,x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "    \n",
        "    \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    #define Multilayer Perceptron\n",
        "    mlp=MLP()\n",
        "    \n",
        "    #create inputs\n",
        "    inputs = np.random.rand(mlp.num_inputs)\n",
        "    \n",
        "    #forward propagation\n",
        "    outputs = mlp.forward_propagation(inputs)\n",
        "    \n",
        "    #print outputs\n",
        "    print(\"inputs are :\", inputs)\n",
        "    print(\"outputs are :\", outputs)\n",
        "    \n",
        "    \n",
        "     \n",
        "            \n",
        "            \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}